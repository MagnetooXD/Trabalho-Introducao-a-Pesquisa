# _Can Autograding of Student-Generated Questions Quality by ChatGPT Match Human Experts?_

Li, Kangkang; Yang, Qian; Yang, Xianmin. "Can Autograding of Student-Generated Questions Quality by ChatGPT Match Human Experts," in IEEE Transactions on Learning Technologies, vol. 17, no. 6, pp. 1600-1610, 2024. doi: [10.1109/TLT.2024.3394807](https://ieeexplore.ieee.org/document/10510637)

## 1. Fichamento de Conteúdo

O artigo _"Can Autograding of Student-Generated Questions Quality by ChatGPT Match Human Experts?"_ examina a eficácia dos modelos de linguagem GPT-3.5 e GPT-4.0 no processo de avaliação automática de perguntas formuladas por alunos, conhecidas como SGQs _(Student-Generated Questions)_. Estas perguntas são importantes no contexto educacional, pois indicam o nível de compreensão dos estudantes, possíveis lacunas de aprendizado e dificuldades específicas nos conteúdos ensinados. O processo de avaliação humana é demorado e custoso, e o estudo explora como a inteligência artificial, particularmente o GPT, poderia aliviar esse desafio. Para isso, o artigo avalia a capacidade dos modelos GPT em julgar cinco dimensões da qualidade das perguntas: relevância, clareza, capacidade de resposta, nível de desafio e nível cognitivo, baseado na taxonomia de Bloom revisada.

Nos experimentos realizados, o GPT-4.0 mostrou resultados mais próximos dos especialistas humanos em critérios como relevância, clareza e capacidade de resposta das perguntas. No entanto, ambos os modelos apresentaram limitações ao avaliar o nível cognitivo, com baixa concordância em relação aos especialistas, o que demonstra uma dificuldade em captar nuances educacionais complexas. A análise incluiu uma comparação com um conjunto de dados chamado LearningQ, com perguntas de outras áreas, para validar a consistência dos modelos em diferentes domínios do conhecimento. Embora os resultados sejam promissores, o estudo conclui que os modelos ainda não são suficientemente precisos para substituir os especialistas humanos. As conclusões ressaltam que, embora o GPT-4.0 possa auxiliar na avaliação preliminar de perguntas e oferecer justificativas úteis, a avaliação completa e precisa do nível cognitivo e das capacidades analíticas dos alunos ainda requer a supervisão e a intervenção de professores e especialistas.

## 2. Fichamento Bibliográfico 

* A estratégia de perguntas geradas por estudantes (SGQ) ajuda a entender a compreensão dos alunos e promove o pensamento crítico e computacional.
* A qualidade das perguntas foi avaliada com base em cinco critérios principais: relevância, clareza, capacidade de resposta, desafio e nível cognitivo, conforme a taxonomia de Bloom.
* GPT-4.0 mostrou maior precisão em comparação com GPT-3.5 na avaliação de relevância e clareza.
* A avaliação automática de perguntas por nível cognitivo foi insuficiente nos modelos GPT-3.5 e GPT-4.0, demonstrando baixa concordância com especialistas.
* Professores e alunos consideram as justificativas de avaliação do GPT úteis, embora ainda haja inconsistências nos níveis cognitivos atribuídos.

## 3. Fichamento de Citações

* _"SGQs indicate information about students’ comprehension level of the lecture, conceptual misunderstandings, or gaps between teaching objectives and students’ understanding."_
* _"GPT-4.0 exhibits superior grading consistency with experts compared to GPT-3.5 in terms of topic relevance, clarity of expression, answerability, and difficulty level."_
* _"However, both GPT-3.5 and GPT-4.0 showed low consistency with experts regarding cognitive level."_
* _"We propose that teachers should refrain from directly employing GPT models for scoring SGQs."_
* _"The findings underscore the potential of GPT-4.0 to assist teachers in evaluating the quality of SGQs."_
